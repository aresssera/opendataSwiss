{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all datasets and the according attribute values which contain (at least) one link without _https_ which also works with the replaced protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests     # 2.18.4\n",
    "import json         # 2.0.9\n",
    "import pandas as pd # 0.23.0\n",
    "import numpy as np\n",
    "import ast\n",
    "from urlextract import URLExtract as extract\n",
    "extractor = extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BFE Abnahme\n",
    "packages = 'https://ckan.ogdch-abnahme.clients.liip.ch/api/3/action/package_search?fq=organization:(bundesamt-fur-energie-bfe)&rows=500'\n",
    "\n",
    "# Make the HTTP request\n",
    "response = requests.get(packages)\n",
    "\n",
    "# Use the json module to load CKAN's response into a dictionary\n",
    "response_dict = json.loads(response.content)\n",
    "\n",
    "# Check the contents of the response\n",
    "assert response_dict['success'] is True  # make sure if response is OK\n",
    "\n",
    "# Get a list of all publications and their information (each publication is a dictionary)\n",
    "data = response_dict['result']['results'].copy()\n",
    "\n",
    "# get all upper keys\n",
    "allKeys = []\n",
    "\n",
    "# store information about each dataset in a frame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigFiles   = pd.read_csv('bigFiles.csv')['Link'].to_list()\n",
    "notWorking = pd.read_csv('notWorking.csv')['Link'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_checker(url):\n",
    "\ttry:\n",
    "\t\t# first check if one of the urls is downloading huge files\n",
    "\t\t# (already tested)\n",
    "\t\t\n",
    "\t\tif url in bigFiles:\n",
    "\t\t\treturn True\n",
    "\t\t\n",
    "\t\tif url in notWorking:\n",
    "\t\t\treturn False\n",
    "\n",
    "\t\t#Get Url\n",
    "\t\tget = requests.get(url)\n",
    "\t\t# if the request succeeds \n",
    "\t\tif get.status_code == 200:\n",
    "\t\t\treturn True\n",
    "\t\telse:\n",
    "\t\t\treturn False\n",
    "\n",
    "\t#Exception\n",
    "\texcept requests.exceptions.RequestException as e:\n",
    "        # print URL with Errs\n",
    "\t\traise SystemExit(f\"{url}: is Not reachable \\nErr: {e}\")\n",
    "\n",
    "def testAdjustedURL(url):\n",
    "    adjusted = url.replace(\"http:\", \"https:\" )\n",
    "    adjusted = adjusted.replace(\"www\", \"https://www\" )\n",
    "    adjusted = adjusted.replace(\"https://https://www\", \"https://www\" )\n",
    "\n",
    "    return [url_checker(adjusted), adjusted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromDictionary(d):\n",
    "\n",
    "    changed  = False\n",
    "\n",
    "    for key in d.keys():\n",
    "\n",
    "        val      = d[key]\n",
    "        isString = True\n",
    "        \n",
    "\n",
    "        if type(val) not in [dict, str, list]:\n",
    "            continue\n",
    "        elif type(val) != str:\n",
    "            isString = False\n",
    "            val = str(val)\n",
    "\n",
    "        urls = extractor.find_urls(val)\n",
    "\n",
    "        if len(urls) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            for url in urls:\n",
    "\n",
    "                working = False\n",
    "\n",
    "                if not any(prefix in url for prefix in ['www', 'http']):\n",
    "                    continue\n",
    "                elif 'https://' in url:\n",
    "                    continue\n",
    "                else:\n",
    "                    #first check if the adjusted url works, then replace it in the original\n",
    "\n",
    "                    working, adjusted_url = testAdjustedURL(url)\n",
    "\n",
    "                    if working:\n",
    "                        changed = True\n",
    "\n",
    "                        if isString:\n",
    "                            d[key] = val.replace(url, adjusted_url)\n",
    "                        else:\n",
    "                            d[key] = ast.literal_eval(val.replace(url, adjusted_url))\n",
    "\n",
    "\n",
    "    if changed:\n",
    "        return [d]\n",
    "    else:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromList(l):\n",
    "\n",
    "    changed  = False\n",
    "\n",
    "    for i in range(len(l)):\n",
    "\n",
    "        val = l[i]\n",
    "        isString = True\n",
    "\n",
    "        if type(val) not in [dict, str, list]:\n",
    "            continue\n",
    "        elif type(val) != str:\n",
    "            isString = False\n",
    "            val = str(val)\n",
    "\n",
    "        urls = extractor.find_urls(val)\n",
    "\n",
    "        if len(urls) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            for url in urls:\n",
    "\n",
    "                working = False\n",
    "\n",
    "                if not any(prefix in url for prefix in ['www', 'http']):\n",
    "                    continue\n",
    "                elif 'https://' in url:\n",
    "                    continue\n",
    "                else:\n",
    "                    #first check if the adjusted url works, then replace it in the original\n",
    "\n",
    "                    working, adjusted_url = testAdjustedURL(url)\n",
    "\n",
    "                    if working:\n",
    "                        changed = True\n",
    "\n",
    "                        if isString:\n",
    "                            l[i] = val.replace(url, adjusted_url)\n",
    "                        else:\n",
    "                            l[i] = ast.literal_eval(val.replace(url, adjusted_url))\n",
    "\n",
    "\n",
    "    if changed:\n",
    "        return [l]\n",
    "    else:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromString(string):\n",
    "     \n",
    "     changed = False\n",
    "     urls = extractor.find_urls(string)\n",
    "     \n",
    "     for url in urls:\n",
    "          \n",
    "          working = False\n",
    "          \n",
    "          if not any(prefix in url for prefix in ['www', 'http']):\n",
    "               continue\n",
    "          elif 'https://' in url:\n",
    "               continue\n",
    "\n",
    "          # change to https://\n",
    "          else:\n",
    "               working, adjusted_url = testAdjustedURL(url)\n",
    "            \n",
    "          \n",
    "          if working:\n",
    "               changed = True\n",
    "               string  = string.replace(url, adjusted_url)\n",
    "               \n",
    "     \n",
    "     if changed:\n",
    "          return [string]\n",
    "     else:\n",
    "          return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInfo(elem):\n",
    "\n",
    "    if type(elem) == dict:\n",
    "        return fromDictionary(elem)\n",
    "\n",
    "    elif type(elem) == list:\n",
    "        return fromList(elem)\n",
    "\n",
    "    elif type(elem) == str:\n",
    "        return fromString(elem)\n",
    "\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns\n",
    "\n",
    "packageIDs      = []\n",
    "packageNames    = []\n",
    "attributeNames  = []\n",
    "attributeValues = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    for col in cols:\n",
    "\n",
    "        value = getInfo(row[col])\n",
    "\n",
    "        if value == []:\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            packageIDs.append(row['id'])\n",
    "            packageNames.append(row['title']['de'])\n",
    "            attributeNames.append(col)\n",
    "            attributeValues = attributeValues + value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame([packageIDs, packageNames, attributeNames, attributeValues]).T\n",
    "frame.columns = ['package_id', 'package_name', 'attribute_name', 'attribute_value']\n",
    "frame.to_csv('linkAndAttributeValues.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
